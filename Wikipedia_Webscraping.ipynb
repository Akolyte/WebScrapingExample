{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZiItfCsxrIX"
   },
   "source": [
    "# Webscraping Tutorial - Using BeautifulSoup to store and process HTML tags in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPB8v08KyEqZ"
   },
   "source": [
    "SODA 308, April 16th, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xagfea2myOCI"
   },
   "source": [
    "Hojin Ryoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pikwYgshyZNn"
   },
   "source": [
    "This tutorial is designed to teach someone how to use BeautifulSoup to scrape information off of webpages. What we obtain first to create our \"soup\" is to get the raw HTML text from the site we wish to scrape. Let's install the packages we need in order to create the soup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ekilsyMkxDB7",
    "outputId": "a8915159-4afa-4752-943c-e1a285c8e3ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\hojin\\anaconda3\\lib\\site-packages (4.9.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hojin\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qkdidyqxy4Ob",
    "outputId": "97f97607-31d0-42ce-9cba-799dfbbfddc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\hojin\\anaconda3\\lib\\site-packages (2.24.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hojin\\anaconda3\\lib\\site-packages (from requests) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\hojin\\anaconda3\\lib\\site-packages (from requests) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\hojin\\anaconda3\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\hojin\\anaconda3\\lib\\site-packages (from requests) (2.10)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Wlfywsf8zSS"
   },
   "source": [
    "Here we are importing the packages we need so we can use them in our code. The packages we will be using are the requests package to get the site raw content, and BeautifulSoup to search for HTML tags. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KUniSuqJP3V7"
   },
   "source": [
    "### Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "CUkZZ_DSzDf7"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jfd133lr9FU0"
   },
   "source": [
    "For this example we will be gathering the top 100 universities ranked by topuniversities.com. In order to do this we define the url and get the raw content of that page and assign it to the variable 'page'. ![base_site](https://drive.google.com/uc?export=view&id=1axalcr7loA15942f6ItPeKupsWFyJCbH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "HCpaiP4CzHZB"
   },
   "outputs": [],
   "source": [
    "url = \"https://www.topuniversities.com/where-to-study/north-america/united-states/ranked-top-100-us-universities\"\n",
    "page = requests.get(url).content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nd0nlfAC9C9D"
   },
   "source": [
    "Now that we have the raw content, we create a soup object from the BeautifulSoup library, and parse it with an html parser. Soups are special in that they have special attributes that you can call for, which can be useful when looking for certain HTML tags. Here we call the title tag of the soup object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mcJuopOs0CkZ",
    "outputId": "a86e6ffe-5af5-462e-baf5-2fb247dee4c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>Ranked: The Top 100 US Universities | Top Universities</title>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(page, 'html.parser')\n",
    "soup.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5g1uP929iJn"
   },
   "source": [
    "It may be helpful to actually use the inspect tool in the browser to see what tags are used to contain the elements you are hoping to scrape from a website. ![inspect](https://drive.google.com/uc?export=view&id=1_M1UCx0rfkfS7A6Fd4lwAtrMcfeAURFN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEsiz6vsBJ_-"
   },
   "source": [
    "After inspecting the website, we see that the university names are under the 'td' tag, which is a common kind of HTML table. We also see that the names are links as well. Therefore we are looking for link elements that are enclosed in the 'td' tag table. We can do this by using the find_all function, which searches the raw HTML tags for a specific tag, in this case 'td'. It will return all 'td' tag elements in a list. Below we show the first 6 results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R7U3ZdzI0JRG",
    "outputId": "0fe8230c-481a-4d9c-9d40-aab726124fa5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<td colspan=\"2\" width=\"389\">\n",
       " <h2 style=\"text-align: center;\"><strong>Top 100 US universities</strong></h2>\n",
       " </td>,\n",
       " <td width=\"44\">\n",
       " <p><strong>Rank</strong></p>\n",
       " </td>,\n",
       " <td width=\"343\">\n",
       " <p><strong>University</strong></p>\n",
       " </td>,\n",
       " <td width=\"44\">\n",
       " <p>Â  1</p>\n",
       " </td>,\n",
       " <td width=\"343\">\n",
       " <p><a href=\"https://www.topuniversities.com/universities/harvard-university\">Harvard University</a></p>\n",
       " </td>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td_list = soup.find_all('td')\n",
    "td_list[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v7KQITHKCN01"
   },
   "source": [
    "After inspecting the results of the function, we see that the elements of the table that are university names are enclosed in the 'a' tag, which is used to identify links. We can use the text attribute of the tag to find the text within the tag. \n",
    "\n",
    "After we find the text of the link, aka the university name, we append it to a list of all the university names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "rtF7H5sU7h2Y"
   },
   "outputs": [],
   "source": [
    "# Gathering Top 100 University Names\n",
    "uni_names = []\n",
    "# For each element of the list of table elements, find if there is a link.\n",
    "for td in td_list:\n",
    "  raw = td.find('a')\n",
    "  # If a link is not returned, continue to the next element. \n",
    "  if raw == None:\n",
    "    continue\n",
    "  # Otherwise, add the text of the link to \n",
    "  else:\n",
    "    uni = raw.text\n",
    "    uni_names.append(uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pepCcCm98T-E",
    "outputId": "06137561-3865-4820-f74f-d6a269febc8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harvard University\n",
      "Stanford University\n",
      "Massachusetts Institute of Technology (MIT)\n",
      "University of California, Berkeley (UCB)\n",
      "Columbia University\n",
      "University of California, Los Angeles (UCLA)\n",
      "Yale University\n",
      "University of Pennsylvania\n",
      "Princeton University\n",
      "Cornell University\n",
      "New York University\n",
      "University of Chicago\n",
      "Duke University\n",
      "Johns Hopkins University\n",
      "University of Southern California\n",
      "Northwestern University\n",
      "Carnegie Mellon University\n",
      "University of Michigan\n",
      "California Institute of Technology (Caltech)\n",
      "Brown University\n",
      "Boston University\n",
      "Rice University\n",
      "Georgetown University\n",
      "University of Washington\n",
      "University of Texas at Austin\n",
      "University of California, San Diego (UCSD)\n",
      "Emory University\n",
      "University of California, Davis\n",
      "Washington University in St. Louis\n",
      "University of Rochester\n",
      "Vanderbilt University\n",
      "Georgia Institute of Technology\n",
      "University of Illinois at Urbana-Champaign\n",
      "George Washington University\n",
      "Tufts University\n",
      "University of Florida\n",
      "Dartmouth College\n",
      "University of North Carolina, Chapel Hill\n",
      "University of Miami\n",
      "University of Notre Dame\n",
      "Rutgers University - New Brunswick\n",
      "University of California, Irvine\n",
      "Case Western Reserve University\n",
      "University of Illinois, Chicago (UIC)\n",
      "Stony Brook University, State University of New York\n",
      "University at Buffalo SUNY\n",
      "Pennsylvania State University\n",
      "Boston College\n",
      "University of Maryland, College Park\n",
      "University of Virginia\n",
      "Syracuse University\n",
      "University of Wisconsin-Madison\n",
      "Purdue University\n",
      "Northeastern University\n",
      "University of Minnesota\n",
      "Michigan State University\n",
      "Brandeis University\n",
      "The Ohio State University\n",
      "Drexel University\n",
      "University of Massachusetts Amherst\n",
      "Temple University\n",
      "North Carolina State University\n",
      "University of Colorado at Boulder\n",
      "Lehigh University\n",
      "Rensselaer Polytechnic Institute\n",
      "University of Hawaii at Manoa\n",
      "Tulane University\n",
      "Howard University\n",
      "University of Arizona\n",
      "University of Maryland, Baltimore County\n",
      "Binghamton University SUNY\n",
      "Illinois Institute of Technology\n",
      "University of Pittsburgh\n",
      "New Jersey Institute of Technology (NJIT)\n",
      "University of California, Santa Barbara (UCSB)\n",
      "University of the Pacific\n",
      "Texas A&M University\n",
      "University of Denver\n",
      "University of Massachusetts, Boston\n",
      "College of William & Mary\n",
      "Florida State University\n",
      "San Diego State University\n",
      "University of Connecticut\n",
      "Virginia Commonwealth University\n",
      "Indiana University Bloomington\n",
      "Worcester Polytechnic Institute\n",
      "University of Houston\n",
      "University of San Francisco\n",
      "The University of Georgia\n",
      "University of Oklahoma, Norman\n",
      "Arizona State University, Tempe\n",
      "Wake Forest University\n",
      "Southern Methodist University\n",
      "Andrews University\n",
      "University of California, Santa Cruz (UCSC)\n",
      "University of Oregon\n",
      "University of Texas Dallas\n",
      "Santa Clara University\n",
      "University of Louisville\n",
      "George Mason University\n",
      "Rutgers - The State University of New Jersey, Newark\n",
      "Virginia Polytechnic Institute (Virginia Tech)\n"
     ]
    }
   ],
   "source": [
    "# Print out the list of top 100 universities\n",
    "for uni in uni_names:\n",
    "  print(uni)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-k5pr0ZpP8_h"
   },
   "source": [
    "### Cleaning Web Sraped Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VQz4AcYDf8E"
   },
   "source": [
    "Scrolling down, we also see that the site organizes the universities by state. Suppose we want to collect the information of how many of the top 100 universities are in each state. We can attempt to collect this information as well by inspecting the site and seeing where the information we want is stored in the HTML tags. ![base_site_2](https://drive.google.com/uc?export=view&id=1ljt3V-TnaSpaK13tttUgGzxkWBGsCmUK) ![inspect_2](https://drive.google.com/uc?export=view&id=1fw7e0UJNERYG_b1vcka-KqfSBoM2p0zJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqBUeFN6GSVf"
   },
   "source": [
    "After inspecting the site we see that the counts and the state name are in the 'strong' tag, which refers to bold text. We can now gather those tags through the find_all function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fcWWQUjWGQMT",
    "outputId": "81f76d7a-b68f-4e57-8085-3f2b33cbc502"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<strong>Top 100 US universities</strong>,\n",
       " <strong>Rank</strong>,\n",
       " <strong>University</strong>,\n",
       " <strong style=\"font-size: 14px;\">\n",
       " <div class=\"media media-element-container media-default\">\n",
       " <div class=\"file file-image\" id=\"file-319658\">\n",
       " <div class=\"content\">\n",
       " <img alt=\"\" class=\"media-element file-default\" height=\"466\" src=\"/sites/default/files/arizonastate.jpg\" style=\"\" title=\"\" width=\"700\"/>\n",
       " </div>\n",
       " </div>\n",
       " </div></strong>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strong_list = soup.find_all('strong')\n",
    "strong_list[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7G88tPGLY21"
   },
   "source": [
    "Then, we will loop through the list of strong tag elements and get the text attribute of element. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "rrhJNzciHBHK"
   },
   "outputs": [],
   "source": [
    "# Gathering the counts of universities of the top 100 in each state\n",
    "state_counts = []\n",
    "# For each strong tag element, scrape the text, and append it to the state_counts list.\n",
    "for s in strong_list:\n",
    "  sc = s.text\n",
    "  state_counts.append(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rXPfu50zHQ6S",
    "outputId": "81e124ad-0a70-4569-d141-cf965f6a1c73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 US universities\n",
      "Rank\n",
      "University\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Alabama - 3Â \n",
      "Alaska â 2 \n",
      "Arizona - 3\n",
      "Arkansas - 1Â \n",
      "California - 37\n",
      "Colorado - 5\n",
      "Connecticut - 5\n",
      "Delaware - 1\n",
      "Florida - 12\n",
      "Georgia - 5\n",
      "Hawaii - 4\n"
     ]
    }
   ],
   "source": [
    "# Printing the list of counts of each state\n",
    "count = 0\n",
    "for sc in state_counts:\n",
    "  print(sc)\n",
    "  count += 1\n",
    "  if count == 15:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zt-YGt5lLu8D"
   },
   "source": [
    "After inspecting the results its clear that something is messed up. Something to note it text isn't always clean when scraped from a website, there may be hidden strings with the backslash escape character '\\' that are hidden within the text. A good way to see this text is using the repr function in your print statements.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "718hMiJiIIa4",
    "outputId": "a7b0e2a2-258e-421c-8624-1b76a13aa9ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Top 100 US universities'\n",
      "'Rank'\n",
      "'University'\n",
      "'\\n\\n\\n\\n\\n\\n\\n'\n",
      "'Alabama - 3\\xa0'\n",
      "'Alaska â 2 '\n",
      "'Arizona - 3'\n",
      "'Arkansas - 1\\xa0'\n",
      "'California - 37'\n",
      "'Colorado - 5'\n",
      "'Connecticut - 5'\n",
      "'Delaware - 1'\n",
      "'Florida - 12'\n",
      "'Georgia - 5'\n",
      "'Hawaii - 4'\n"
     ]
    }
   ],
   "source": [
    "# Printing the list of counts of each state\n",
    "count = 0\n",
    "for sc in state_counts:\n",
    "  print(repr(sc))\n",
    "  count += 1\n",
    "  if count == 15:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2tI9SrhMCBU"
   },
   "source": [
    "Upon further review, we see that there are some new line strings, as well as some extraneous escape characters at the end of our strings. Cleaning text is fairly common in web scraping, so we will go through this example of that process now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "zvgTiyjXNps8"
   },
   "outputs": [],
   "source": [
    "# Removing extraneous escape characters and strings from the state counts list. \n",
    "state_counts_clean = []\n",
    "for sc in state_counts:\n",
    "  # Remove newline escape character\n",
    "  newline = sc.replace('\\n', '')\n",
    "  # Remove \\xa0 character\n",
    "  tag = newline.replace('\\xa0', '')\n",
    "  # If after cleaning the string is empty, do not append it\n",
    "  if tag == \"\":\n",
    "    continue\n",
    "  # Otherwise, append the string to a new clean counts list\n",
    "  else:\n",
    "    state_counts_clean.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w6odFDshN8I3",
    "outputId": "de2ad36f-2d84-48b4-8e5d-d29d78135110"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Alabama - 3'\n",
      "'Alaska â 2 '\n",
      "'Arizona - 3'\n",
      "'Arkansas - 1'\n",
      "'California - 37'\n",
      "'Colorado - 5'\n",
      "'Connecticut - 5'\n",
      "'Delaware - 1'\n",
      "'Florida - 12'\n",
      "'Georgia - 5'\n",
      "'Hawaii - 4'\n",
      "'Idaho - '\n",
      "'2 '\n",
      "'Illinois - 12'\n",
      "'Indiana - 8'\n"
     ]
    }
   ],
   "source": [
    "# Printing the results:\n",
    "# Removing the extraneous results using string splicing.\n",
    "prelim = state_counts_clean[3:-2]\n",
    "count = 0\n",
    "for scc in prelim:\n",
    "  print(repr(scc))\n",
    "  count += 1\n",
    "  if count == 15:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Co2g-2chSlqN"
   },
   "source": [
    "After this round of cleaning, and removing some of the elements of the list through slicing, we have a couple entries where the number and the state are separate. How do we take care of this issue? \n",
    "\n",
    "This format of states being assigned numbers lends itself very well to a dictionary format. Therefore we will make a dictionary, to organize the data into clean key-value pairs. The keys will be the state name and the values will be the number of schools that are in the top 100 that are in that state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Sw2_1BYZJKnA"
   },
   "outputs": [],
   "source": [
    "school_counts = {}\n",
    "for scc in prelim:\n",
    "  # Splitting the string into the state names and the counts.\n",
    "  splitted1 = scc.split('-')\n",
    "  splitted2 = scc.split('â')\n",
    "  # Checking which split operator actually splits the string, and accepting the one that does.\n",
    "  if len(splitted1) == 2:\n",
    "    splitted = splitted1\n",
    "  elif len(splitted2) == 2:\n",
    "    splitted = splitted2\n",
    "  else:\n",
    "    continue\n",
    "  # Removing extra spaces from the string.\n",
    "  p1 = splitted[0].strip()\n",
    "  p2 = splitted[1].strip()\n",
    "  # Accounting for two exceptions.\n",
    "  if p1 == 'Idaho':\n",
    "    school_counts[p1] = 2\n",
    "  elif p1 == 'Virginia':\n",
    "    school_counts[p1] = 10\n",
    "  else:\n",
    "    school_counts[p1] = p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02j6-_E6KdhC",
    "outputId": "e99413d8-a136-4721-9866-8add2cafe3c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alabama:(3)\n",
      "Alaska:(2)\n",
      "Arizona:(3)\n",
      "Arkansas:(1)\n",
      "California:(37)\n",
      "Colorado:(5)\n",
      "Connecticut:(5)\n",
      "Delaware:(1)\n",
      "Florida:(12)\n",
      "Georgia:(5)\n",
      "Hawaii:(4)\n",
      "Idaho:(2)\n",
      "Illinois:(12)\n",
      "Indiana:(8)\n",
      "Iowa:(3)\n",
      "Kansas:(2)\n",
      "Kentucky:(2)\n",
      "Louisiana:(2)\n",
      "Maine:(0)\n",
      "Maryland:(3)\n",
      "Massachusetts:(16)\n",
      "Michigan:(7)\n",
      "Minnesota:(2)\n",
      "Mississippi:(2)\n",
      "Missouri:(6)\n",
      "Montana:(2)\n",
      "Nebraska:(2)\n",
      "Nevada:(2)\n",
      "New Hampshire:(2)\n",
      "New Jersey:(12)\n",
      "New Mexico:(2)\n",
      "New York:(36)\n",
      "North Carolina:(8)\n",
      "North Dakota:(0)\n",
      "Ohio:(5)\n",
      "Oklahoma:(4)\n",
      "Oregon:(4)\n",
      "Pennsylvania:(15)\n",
      "Rhode Island:(3)\n",
      "South Carolina:(2)\n",
      "South Dakota:(1)\n",
      "Tennessee:(3)\n",
      "Texas:(18)\n",
      "Utah:(2)\n",
      "Vermont:(1)\n",
      "Virginia:(10)\n",
      "Washington:(6)\n",
      "Washington D.C.:(6)\n",
      "West Virginia:(1)\n",
      "Wisconsin:(4)\n",
      "Wyoming:(1)\n"
     ]
    }
   ],
   "source": [
    "for key in school_counts:\n",
    "  print(\"{}:({})\".format(key, school_counts[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXwooTnaPaZ0"
   },
   "source": [
    "And there you have it! You have now learned how to scrape a webpage using BeautifulSoup, and you have some ideas on how to clean the data that you are able to gather."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpsXS3awRd1z"
   },
   "source": [
    "### Web Scraping Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuBOc16oRl3q"
   },
   "source": [
    "Web scraping is very useful for research and information gathering purposes. Copying and pasting over information from websites can be a long and difficult task, especially if there is a lot of data to be collected. It helps to automate that process and make it simple and easy to collect information from websites that you may require."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_z7vHyzT0Mx"
   },
   "source": [
    "In terms of research projects Web Scraping should be used when you need a lot of information from an online database or table, that is too massive in volume or has awkward formatting that makes it difficult to just copy directly. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Wikipedia_Webscraping.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
